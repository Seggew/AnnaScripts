import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import adjusted_rand_score
from sklearn.model_selection import ParameterGrid
import itertools

# Load your ground truth and data
data = pd.read_feather("/Users/seggewa/Desktop/cluster/1hrTRIM2024-08-29_14-01-33_SV2.tracks.feather")  # Load the data file
ground_truth = pd.read_csv("/Users/seggewa/Desktop/gridsearch/gtLOWS2.csv")  # Load the ground truth file

# Make sure both data and ground truth have a 'frame' and 'cluster' column for comparison
data['frame'] = data['frame'].astype(int)
ground_truth['frame'] = ground_truth['frame'].astype(int)
gt_clusters = ground_truth[ground_truth['frame'] == frame]['cluster'].values


filtered_data = data[(data['frame'] >= 1) & (data['frame'] <= 1)]

filtered_data = filtered_data.dropna(subset=['x_head', 'y_head', 'x_tail', 'y_tail'])
# Calculate vectors (tail - head) for each instance as before
filtered_data['vector_x'] = filtered_data['x_tail'] - filtered_data['x_head']
filtered_data['vector_y'] = filtered_data['y_tail'] - filtered_data['y_head']

# Define custom distance function with eps and cosine similarity threshold
def custom_distance(A, B, eps, cosine_threshold):
    # Euclidean distance between tail coordinates
    tail_A = A[:2]
    tail_B = B[:2]
    euclidean_tail_dist = np.linalg.norm(tail_A - tail_B)
    
    # Vector-based cosine similarity
    vector_A = A[2:]
    vector_B = B[2:]
    magnitude_A = np.linalg.norm(vector_A)
    magnitude_B = np.linalg.norm(vector_B)
    
    # Avoid division by zero
    if magnitude_A == 0 or magnitude_B == 0:
        return 1000  # Treat zero-magnitude vectors as far apart
    
    cos_similarity = np.dot(vector_A, vector_B) / (magnitude_A * magnitude_B)
    
    # Apply both conditions independently
    if euclidean_tail_dist <= eps and cos_similarity > cosine_threshold:
        return euclidean_tail_dist
    return 1000  # Large distance if either condition fails

# Define parameter grid for eps and cosine similarity threshold
param_grid = {
    'eps': [25, 30, 35, 40, 45, 50, 55, 60],  # Adjust eps values as needed
    'cosine_threshold': [0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.866, 0.9]  # Cosine values for angles of 60, 45, 30, and closer
}

# Initialize a dictionary to store ARI scores
grid_search_results = []

# Loop through each frame for grid search
for frame, group in filtered_data.groupby('frame'):
    # Extract the ground truth clusters for this frame
    gt_clusters = ground_truth[ground_truth['frame'] == frame]['cluster'].values
    
    # Prepare data for clustering with [x_tail, y_tail, vector_x, vector_y]
    vectors = group[['x_tail', 'y_tail', 'vector_x', 'vector_y']].values
    
    # Perform grid search over eps and cosine_threshold
    for params in ParameterGrid(param_grid):
        # Apply DBSCAN with the current parameters and custom metric
        dbscan = DBSCAN(eps=params['eps'], min_samples=3, metric=lambda A, B: custom_distance(A, B, params['eps'], params['cosine_threshold']))
        labels = dbscan.fit_predict(vectors)
        
        # Calculate ARI score against the ground truth
        ari_score = adjusted_rand_score(gt_clusters, labels)
        
        # Store the results
        grid_search_results.append({
            'frame': frame,
            'eps': params['eps'],
            'cosine_threshold': params['cosine_threshold'],
            'ari_score': ari_score
        })

# Convert results to a DataFrame and find the best parameters
results_df = pd.DataFrame(grid_search_results)
best_params = results_df.loc[results_df['ari_score'].idxmax()]
print("Best parameters:", best_params)

# Save the results to a CSV for analysis
results_df.to_csv('/Users/seggewa/Desktop/gridsearch/grid_search_resultsSVLOW.csv', index=False)
print("Grid search results saved.")
