import pandas as pd
import numpy as np
from sklearn.cluster import DBSCAN
from sklearn.metrics import adjusted_rand_score
from sklearn.model_selection import ParameterGrid

# Load your ground truth and data
data = pd.read_feather("/Users/seggewa/Desktop/ExtractFrames/2024-08-28_12-19-14_SV13.predictions.feather")  # Load the data file
ground_truth = pd.read_csv("/Users/seggewa/Desktop/ExtractFrames/GT45.9-2024-08-28_12-19-14_SV13.csv")  # Load the ground truth file

# Make sure both data and ground truth have a 'frame' and 'cluster' column for comparison
data['frame'] = data['frame'].astype(int)
ground_truth['frame'] = ground_truth['frame'].astype(int)

# Filter data for frame 1 only and drop rows with missing values
filtered_data = data[data['frame'] == 75223].dropna(subset=['x_head', 'y_head', 'x_tail', 'y_tail'])
ground_truth_frame = ground_truth[ground_truth['frame'] == 75223]

# Sort both datasets by the x_tail coordinate for alignment
filtered_data = filtered_data.sort_values(by='x_tail').reset_index(drop=True)
ground_truth_frame = ground_truth_frame.sort_values(by='x_tail').reset_index(drop=True)

# Calculate vectors (tail - head) for each instance in filtered_data
filtered_data['vector_x'] = filtered_data['x_tail'] - filtered_data['x_head']
filtered_data['vector_y'] = filtered_data['y_tail'] - filtered_data['y_head']

# Define custom distance function with eps and cosine similarity threshold
def custom_distance(A, B, eps, cosine_threshold):
    tail_A = A[:2]
    tail_B = B[:2]
    euclidean_tail_dist = np.linalg.norm(tail_A - tail_B)
    
    vector_A = A[2:]
    vector_B = B[2:]
    magnitude_A = np.linalg.norm(vector_A)
    magnitude_B = np.linalg.norm(vector_B)
    
    if magnitude_A == 0 or magnitude_B == 0:
        return 1000  # Treat zero-magnitude vectors as far apart
    
    cos_similarity = np.dot(vector_A, vector_B) / (magnitude_A * magnitude_B)
    
    if euclidean_tail_dist <= eps and cos_similarity > cosine_threshold:
        return euclidean_tail_dist
    return 1000  # Large distance if either condition fails

# Define parameter grid for eps and cosine similarity threshol
param_grid = {
    'eps': np.round(np.linspace(10, 100, 50), 2),  # 50 values for eps between 10 and 100, rounded to 2 decimals
    'cosine_threshold': np.round(np.linspace(0, 1, 50), 2)  # 50 values for cosine_threshold between 0 and 1, rounded to 2 decimals
}


# Initialize a list to store ARI scores and parameter results
grid_search_results = []

# Loop through each frame for grid search (only frame 1 here)
vectors = filtered_data[['x_tail', 'y_tail', 'vector_x', 'vector_y']].values
gt_clusters = ground_truth_frame['cluster'].values  # Ground truth clusters, now sorted by x_tail

for params in ParameterGrid(param_grid):
    dbscan = DBSCAN(eps=params['eps'], min_samples=3, metric=lambda A, B: custom_distance(A, B, params['eps'], params['cosine_threshold']))
    labels = dbscan.fit_predict(vectors)
    
    ari_score = adjusted_rand_score(gt_clusters, labels)
    
    grid_search_results.append({
        'frame': 75223,
        'eps': params['eps'],
        'cosine_threshold': params['cosine_threshold'],
        'ari_score': ari_score
    })

# Convert results to a DataFrame and identify best and worst parameters
results_df = pd.DataFrame(grid_search_results)
best_params = results_df.loc[results_df['ari_score'].idxmax()]
worst_params = results_df.loc[results_df['ari_score'].idxmin()]
print("Best parameters:", best_params)
print("Worst parameters:", worst_params)

# Save the grid search results to a CSV for analysis
results_df.to_csv('/Users/seggewa/Desktop/ARIproof/ARI2024-08-28_12-19-14_SV13.csv', index=False)
print("Grid search results saved.")

# Function to apply DBSCAN with specified parameters, select specific columns, and save results
def run_dbscan_and_save(group, params, output_path):
    vectors = group[['x_tail', 'y_tail', 'vector_x', 'vector_y']].values
    dbscan = DBSCAN(eps=params['eps'], min_samples=3, metric=lambda A, B: custom_distance(A, B, params['eps'], params['cosine_threshold']))
    labels = dbscan.fit_predict(vectors)
    group['cluster'] = labels
    
    # Select only the specified columns
    selected_columns = ['track_id', 'frame', 'x_head', 'y_head', 'x_tail', 'y_tail', 'vector_x', 'vector_y', 'cluster']
    group = group[selected_columns]
    
    # Sort by x_tail for consistency before saving
    group = group.sort_values(by='x_tail')
    
    group.to_csv(output_path, index=False)
    print(f"Results saved to {output_path}")

# # Run DBSCAN with the best parameters and save to CSV
# best_output_path = '/Users/seggewa/Desktop/ARIproof34_SV20/SORTTESTbest_performance.csv'
# run_dbscan_and_save(filtered_data, best_params, best_output_path)

# # Run DBSCAN with the worst parameters and save to CSV
# worst_output_path = '/Users/seggewa/Desktop/ARIproof/2024-08-23_11-24-03_SV18SORTTESTworst_performance.csv'
# run_dbscan_and_save(filtered_data, worst_params, worst_output_path)
